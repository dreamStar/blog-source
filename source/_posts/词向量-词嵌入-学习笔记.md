---
title: 词向量(词嵌入)学习笔记
date: 2017-06-04 23:43:11
tags:
- nlp
- embedding
categories:
- 深度学习与智能
---
简单来说，词嵌入就是把一个词表示成一个（稠密）向量。词转向量的自然方式是使用one-hot形式，但是这种形式的问题在于每个词向量尺寸很大，极其稀疏；同时两个词之间的关系完全无法捕捉。使用词嵌入（word embedding）之后，相近的词在特征空间上的距离就更近。

one-hot转词嵌入需要一个词嵌入字典，将词的one-hot向量乘以这个字典就得到了词嵌入向量（所以这里或许可以优化？）。可以看到这相当于是计算了一层隐藏层的activation。词嵌入字典的得到是一个训练任务的”副产品“，词嵌入字典就相当于是网络结构中的一层隐藏层的权值。

词嵌入的做法有很多，不过google的word2vec是当前最流行的，tensorflow中有相关教程。其算法简单来说，就是在输入的连续句子语料中，以一个窗口截取每段话，将这段话中间的词替换为若干个其他词，让网络估计中间的词是原词的概率是多少，是其他词（噪音）的概率是多少，要求前者尽可能低，后者尽可能高。比如

the quick brown fox jumped over the lazy dog

这段话，使用尺寸为1的窗口，那么就会成为如下的样本集：
([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...
词嵌入模型可以正向来（从context猜target），也可以反向来（从target猜context）。skip-gram模型就是反向来（好处是？），于是数据集变成了：
(quick, the), (quick, brown), (brown, quick), (brown, fox), ...
然后便可以进行计算。计算时对于每个样本，要随机采样若干个噪音样本，计算网络对于正确词和噪音词的概率预测，然后使用noise-contrastive estimation (NCE)损失函数（就是把所有结果的交叉熵代价函数相加）作为代价函数，进行训练，最后取出字典。

 在实际使用中,词嵌入可能不仅仅用于nlp.在推荐等场合,"词"嵌入的第一维可能非常大,以致于在单机中无法存放.所幸的是我们在计算中不需要用到全部词向量,因此,可以把embedding字典分布式存储起来,仅在计算时取对应的向量,计算完成后将梯度信息再送回分布式服务器进行更新.
